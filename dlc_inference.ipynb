{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e5dee6-72b5-4481-a533-75ccc7c11afc",
   "metadata": {},
   "source": [
    "## Run inference on batch of videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c282bf2-ae1a-4636-b4d2-a02b0846198e",
   "metadata": {},
   "source": [
    "### Setup the run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a17c150-26d6-4e13-829d-f9c8b1a17c5d",
   "metadata": {},
   "source": [
    "The first step is to setup the GPU we'll use to run the inference. With the GPU, inference can take ~30 minutes per 10 minute video, and can be much slower without it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36f56f5-d959-4e40-965a-288219a1aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053dd601-c824-4249-971d-9b80b694bca0",
   "metadata": {},
   "source": [
    "The following command will display GPU usage. We can see which are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc752493-f2d4-4c7b-9ec5-2c6497a7f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 17 07:07:54 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:81:00.0 Off |                  Off |\n",
      "| 30%   30C    P8    19W / 300W |  47975MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   31C    P8    22W / 300W |      6MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA T1000        On   | 00000000:C2:00.0 Off |                  N/A |\n",
      "| 33%   33C    P8    N/A /  50W |      6MiB /  3911MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4847      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A   2612128      C   .../envs/fish_env/bin/python    47967MiB |\n",
      "|    1   N/A  N/A      4847      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A      4847      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5487548e-91b8-4899-afae-dfee910a31e3",
   "metadata": {},
   "source": [
    "#### * Choose a free GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d97082a-3cf4-46db-9c62-47da8d7b4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf2468-394f-4289-8dc6-c9064b200feb",
   "metadata": {},
   "source": [
    "Now we import the packages we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc60c69d-1bd2-4473-817a-4eefbdd90a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "#import behaveml\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382cb8f-67f2-4682-b92a-0b639085c2a0",
   "metadata": {},
   "source": [
    "#### * Update paths\n",
    "\n",
    "Set these paths to where the DLC project was placed, the videos you want to run inference on are, and the pattern that will be used to find the video files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e804fcc-ceef-45f4-acd2-e2283baca873",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_file = '/home/blansdel/projects/brett_dlc/animal_pilot/dlc_trained_models/pilot_study-Brett-2021-09-24/config.yaml'\n",
    "video_path = '/home/blansdel/projects/brett_dlc/animal_pilot/overnight_videos_sample/'\n",
    "video_pattern = 'one_minute_clips_000.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078caa11-964b-4dd7-be9b-3f2703c7fc03",
   "metadata": {},
   "source": [
    "Settings -- change if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2724de29-7ec6-43e2-b531-739563187af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_video = False        # Whether or not to plot detections\n",
    "shuffle = 1                 # Which shuffle (model version) to use\n",
    "overwrite = False           # Whether to overwrite current \n",
    "track_method = 'ellipse'    # Which tracker to use (ellipse or box). Ellipse is the default, and generally performs better\n",
    "n_animals = 1               # Number of animals to track (can be different to what the model was trained on)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720103f5-db29-4c44-890a-a16474ad442a",
   "metadata": {},
   "source": [
    "Now make the list of videos we'll be processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ccfaea43-0ae3-4716-9244-a5e5fbbd7f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = glob(os.path.join(video_path, video_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a209a86-8579-431f-bf84-ffe06debf739",
   "metadata": {},
   "source": [
    "Double check things look good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb684255-5580-4f23-9bb1-bc33ab708632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/blansdel/projects/brett_dlc/animal_pilot/overnight_videos_sample/one_minute_clips_000.mp4']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0afc1fe-79fa-4988-9e1d-e85d0d271fe0",
   "metadata": {},
   "source": [
    "### Run the inference\n",
    "\n",
    "May take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "701bee21-517b-46d2-b6f0-08cc70846524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-100000 for model /home/blansdel/projects/brett_dlc/animal_pilot/dlc_trained_models/pilot_study-Brett-2021-09-24/dlc-models/iteration-4/pilot_studySep24-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DEEPLABCUT/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/opt/conda/envs/DEEPLABCUT/lib/python3.8/site-packages/tf_slim/layers/layers.py:684: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  outputs = layer.apply(inputs, training=is_training)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating extracting of PAFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 07:35:29.556432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46717 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:c1:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  /home/blansdel/projects/brett_dlc/animal_pilot/overnight_videos_sample/one_minute_clips_000.mp4\n",
      "Video already analyzed! /home/blansdel/projects/brett_dlc/animal_pilot/overnight_videos_sample/one_minute_clips_000DLC_dlcrnetms5_pilot_studySep24shuffle1_100000.h5\n",
      "Using snapshot-100000 for model /home/blansdel/projects/brett_dlc/animal_pilot/dlc_trained_models/pilot_study-Brett-2021-09-24/dlc-models/iteration-4/pilot_studySep24-trainset95shuffle1\n",
      "Processing...  /home/blansdel/projects/brett_dlc/animal_pilot/overnight_videos_sample/one_minute_clips_000.mp4\n",
      "Tracklets already computed /home/blansdel/projects/brett_dlc/animal_pilot/overnight_videos_sample/one_minute_clips_000DLC_dlcrnetms5_pilot_studySep24shuffle1_100000_el.pickle\n",
      "Set overwrite = True to overwrite.\n",
      "The tracklets were created (i.e., under the hood deeplabcut.convert_detections2tracklets was run). Now you can 'refine_tracklets' in the GUI, or run 'deeplabcut.stitch_tracklets'.\n",
      "Processing...  /home/blansdel/projects/brett_dlc/animal_pilot/overnight_videos_sample/one_minute_clips_000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15768.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The videos are analyzed. Time to assemble animals and track 'em... \n",
      " Call 'create_video_with_all_detections' to check multi-animal detection quality before tracking.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.analyze_videos(path_config_file, videos, videotype='.avi', n_tracks = n_animals)\n",
    "if create_video:\n",
    "    deeplabcut.create_video_with_all_detections(path_config_file, videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a9aec6-13ed-4c0d-af19-095a29817170",
   "metadata": {},
   "source": [
    "Filter predictions to smooth the tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c68592ac-1647-45b0-a960-5228ec8096e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...  /home/blansdel/projects/brett_dlc/animal_pilot/overnight_videos_sample/one_minute_clips_000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21183.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering with median model /home/blansdel/projects/brett_dlc/animal_pilot/overnight_videos_sample/one_minute_clips_000.mp4\n",
      "Saving filtered csv poses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.stitch_tracklets(path_config_file, videos, n_tracks = n_animals)\n",
    "deeplabcut.filterpredictions(path_config_file, videos, windowlength = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6e854-b7e9-4a89-9d7d-42ff2066ce57",
   "metadata": {},
   "source": [
    "Make a video of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cf2be40-d6fa-4120-858b-a87fa4ce6afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process video: /home/blansdel/projects/brett_dlc/animal_pilot/overnight_videos_sample/one_minute_clips_000.mp4\n",
      "Loading /home/blansdel/projects/brett_dlc/animal_pilot/overnight_videos_sample/one_minute_clips_000.mp4 and data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DEEPLABCUT/lib/python3.8/site-packages/deeplabcut/utils/make_labeled_video.py:592: FutureWarning: The behavior of indexing on a MultiIndex with a nested sequence of labels is deprecated and will change in a future version. `series.loc[label, sequence]` will raise if any members of 'sequence' or not present in the index's second level. To retain the old behavior, use `series.index.isin(sequence, level=1)`\n",
      "  df = df.loc(axis=1)[:, individuals]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of video [s]: 60.0, recorded with 30.0 fps!\n",
      "Overall # of frames: 1800 with cropped frame dimensions: 1600 1200\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:33<00:00, 54.19it/s]\n"
     ]
    }
   ],
   "source": [
    "if not create_video:\n",
    "    deeplabcut.create_labeled_video(path_config_file, videos, filtered = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4609eb1-6509-4eba-9e04-cef8263fcc7d",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266eb5f-2ab2-4d19-b121-8aa0e9bda83d",
   "metadata": {},
   "source": [
    "Can access help for a function by following the command with ?\n",
    "e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84b005e0-f46a-42de-839a-ded243494918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvideotype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'avi'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrainingsetindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgputouse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_as_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdestfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcropping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mTFGPUinference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdynamic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodelprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrobust_nframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_shelve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mauto_track\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_tracks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcalibrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0midentity_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "   Makes prediction based on a trained network. The index of the trained network is specified by parameters in the config file (in particular the variable 'snapshotindex')\n",
       "\n",
       "   Output: The labels are stored as MultiIndex Pandas Array, which contains the name of the network, body part name, (x, y) label position \n",
       "\n",
       "           in pixels, and the likelihood for each frame per body part. These arrays are stored in an efficient Hierarchical Data Format (HDF) \n",
       "\n",
       "           in the same directory, where the video is stored. However, if the flag save_as_csv is set to True, the data can also be exported in \n",
       "\n",
       "           comma-separated values format (.csv), which in turn can be imported in many programs, such as MATLAB, R, Prism, etc.\n",
       "\n",
       "   Parameters\n",
       "   ----------\n",
       "   config: string\n",
       "       Full path of the config.yaml file as a string.\n",
       "\n",
       "   videos: list\n",
       "       A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.\n",
       "\n",
       "   videotype: string, optional\n",
       "       Checks for the extension of the video in case the input to the video is a directory.\n",
       "Only videos with this extension are analyzed. The default is ``.avi``\n",
       "\n",
       "   shuffle: int, optional\n",
       "       An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.\n",
       "\n",
       "   trainingsetindex: int, optional\n",
       "       Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).\n",
       "\n",
       "   gputouse: int, optional. Natural number indicating the number of your GPU (see number in nvidia-smi). If you do not have a GPU put None.\n",
       "   See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries\n",
       "\n",
       "   save_as_csv: bool, optional\n",
       "       Saves the predictions in a .csv file. The default is ``False``; if provided it must be either ``True`` or ``False``\n",
       "\n",
       "   destfolder: string, optional\n",
       "       Specifies the destination folder for analysis data (default is the path of the video). Note that for subsequent analysis this\n",
       "       folder also needs to be passed.\n",
       "\n",
       "   batchsize: int, default from pose_cfg.yaml\n",
       "       Change batch size for inference; if given overwrites value in pose_cfg.yaml\n",
       "\n",
       "   cropping: list, optional (default=None)\n",
       "       List of cropping coordinates as [x1, x2, y1, y2].\n",
       "       Note that the same cropping parameters will then be used for all videos.\n",
       "       If different video crops are desired, run 'analyze_videos' on individual videos\n",
       "       with the corresponding cropping coordinates.\n",
       "\n",
       "   TFGPUinference: bool, default: True\n",
       "       Perform inference on GPU with TensorFlow code. Introduced in \"Pretraining boosts out-of-domain robustness for pose estimation\" by\n",
       "       Alexander Mathis, Mert Yüksekgönül, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis Source: https://arxiv.org/abs/1909.11229\n",
       "\n",
       "   dynamic: triple containing (state, detectiontreshold, margin)\n",
       "       If the state is true, then dynamic cropping will be performed. That means that if an object is detected (i.e. any body part > detectiontreshold),\n",
       "       then object boundaries are computed according to the smallest/largest x position and smallest/largest y position of all body parts. This  window is\n",
       "       expanded by the margin and from then on only the posture within this crop is analyzed (until the object is lost, i.e. <detectiontreshold). The\n",
       "       current position is utilized for updating the crop window for the next frame (this is why the margin is important and should be set large\n",
       "       enough given the movement of the animal).\n",
       "\n",
       "   robust_nframes: bool, optional (default=False)\n",
       "       Evaluate a video's number of frames in a robust manner.\n",
       "       This option is slower (as the whole video is read frame-by-frame),\n",
       "       but does not rely on metadata, hence its robustness against file corruption.\n",
       "\n",
       "   allow_growth: bool, default false.\n",
       "       For some smaller GPUs the memory issues happen. If true, the memory allocator does not pre-allocate the entire specified\n",
       "       GPU memory region, instead starting small and growing as needed. See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2\n",
       "\n",
       "   use_shelve: bool, optional (default=False)\n",
       "       By default, data are dumped in a pickle file at the end of the video analysis.\n",
       "       Otherwise, data are written to disk on the fly using a \"shelf\"; i.e., a pickle-based,\n",
       "       persistent, database-like object by default, resulting in constant memory footprint.\n",
       "\n",
       "   The following parameters are only relevant for multi-animal projects:\n",
       "\n",
       "   auto_track: bool, optional (default=True)\n",
       "       By default, tracking and stitching are automatically performed, producing the final h5 data file.\n",
       "       This is equivalent to the behavior for single-animal projects.\n",
       "\n",
       "       If False, one must run `convert_detections2tracklets` and `stitch_tracklets` afterwards, in order to obtain the h5 file.\n",
       "\n",
       "   This function has 3 related sub-calls:\n",
       "\n",
       "   identity_only: bool, optional (default=False)\n",
       "       If True and animal identity was learned by the model,\n",
       "       assembly and tracking rely exclusively on identity prediction.\n",
       "\n",
       "   calibrate: bool, optional (default=False)\n",
       "       If True, use training data to calibrate the animal assembly procedure.\n",
       "       This improves its robustness to wrong body part links,\n",
       "       but requires very little missing data.\n",
       "\n",
       "   n_tracks : int, optional\n",
       "       Number of tracks to reconstruct. By default, taken as the number\n",
       "       of individuals defined in the config.yaml. Another number can be\n",
       "       passed if the number of animals in the video is different from\n",
       "       the number of animals the model was trained on.\n",
       "\n",
       "   Examples\n",
       "   --------\n",
       "\n",
       "   Windows example for analyzing 1 video\n",
       "   >>> deeplabcut.analyze_videos('C:\\myproject\\reaching-task\\config.yaml',['C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi'])\n",
       "   --------\n",
       "\n",
       "   If you want to analyze only 1 video\n",
       "   >>> deeplabcut.analyze_videos('/analysis/project/reaching-task/config.yaml',['/analysis/project/videos/reachingvideo1.avi'])\n",
       "   --------\n",
       "\n",
       "   If you want to analyze all videos of type avi in a folder:\n",
       "   >>> deeplabcut.analyze_videos('/analysis/project/reaching-task/config.yaml',['/analysis/project/videos'],videotype='.avi')\n",
       "   --------\n",
       "\n",
       "   If you want to analyze multiple videos\n",
       "   >>> deeplabcut.analyze_videos('/analysis/project/reaching-task/config.yaml',['/analysis/project/videos/reachingvideo1.avi','/analysis/project/videos/reachingvideo2.avi'])\n",
       "   --------\n",
       "\n",
       "   If you want to analyze multiple videos with shuffle = 2\n",
       "   >>> deeplabcut.analyze_videos('/analysis/project/reaching-task/config.yaml',['/analysis/project/videos/reachingvideo1.avi','/analysis/project/videos/reachingvideo2.avi'],shuffle=2)\n",
       "\n",
       "   --------\n",
       "   If you want to analyze multiple videos with shuffle = 2 and save results as an additional csv file too\n",
       "   >>> deeplabcut.analyze_videos('/analysis/project/reaching-task/config.yaml',['/analysis/project/videos/reachingvideo1.avi','/analysis/project/videos/reachingvideo2.avi'],shuffle=2,save_as_csv=True)\n",
       "   --------\n",
       "\n",
       "   \n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/envs/DEEPLABCUT/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/predict_videos.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deeplabcut.analyze_videos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900cda5-3638-4390-b518-eadbb0bd3465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEEPLABCUT",
   "language": "python",
   "name": "deeplabcut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
